{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_BASE = \"../fakenewsnet_dataset\"\n",
    "NEWS_PATH = f\"{DATASET_BASE}/politifact/fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_df = pd.read_csv(\"../exported/train_ids.csv\", header = 0)\n",
    "cv_id_df = pd.read_csv(\"../exported/cv_ids.csv\", header = 0)\n",
    "test_id_df = pd.read_csv(\"../exported/test_ids.csv\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "uids = []\n",
    "\n",
    "# text\n",
    "descriptions = []\n",
    "locations = []\n",
    "\n",
    "# numeric\n",
    "followers_counts = []\n",
    "friends_counts = []\n",
    "listed_counts = []\n",
    "favorites_counts = []\n",
    "statuses_counts = []\n",
    "\n",
    "# boolean\n",
    "protecteds = []\n",
    "geo_enableds = []\n",
    "verifieds = []\n",
    "\n",
    "for idx, row in train_id_df.iterrows():\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "    \n",
    "    with open(f\"{NEWS_PATH}/{row['news_id']}/tweets/{row['source_tweet_id']}.json\", \"r\") as f:\n",
    "        t = json.loads(f.read())\n",
    "        u = t['user']\n",
    "        \n",
    "        uids.append(u['id'])\n",
    "        descriptions.append(u['description'])\n",
    "        locations.append(u['location'])\n",
    "        followers_counts.append(u['followers_count'])\n",
    "        friends_counts.append(u['friends_count'])\n",
    "        listed_counts.append(u['listed_count'])\n",
    "        favorites_counts.append(u['favourites_count'])\n",
    "        statuses_counts.append(u['statuses_count'])\n",
    "        protecteds.append(u['protected'])\n",
    "        geo_enableds.append(u['geo_enabled'])\n",
    "        verifieds.append(u['verified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, des in enumerate(descriptions):\n",
    "    descriptions[idx] = des.replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \")\n",
    "    \n",
    "for idx, loc in enumerate(locations):\n",
    "    locations[idx] = loc.replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_df['uid'] = uids\n",
    "train_id_df['description'] = descriptions\n",
    "train_id_df['location'] = locations\n",
    "train_id_df['protected'] = protecteds\n",
    "train_id_df['geo_enabled'] = geo_enableds\n",
    "train_id_df['verified'] = verifieds\n",
    "train_id_df['followers_count'] = followers_counts\n",
    "train_id_df['friends_count'] = friends_counts\n",
    "train_id_df['listed_count'] = listed_counts\n",
    "train_id_df['favorites_count'] = favorites_counts\n",
    "train_id_df['statuses_count'] = statuses_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = train_id_df[[\n",
    "    'news_id', 'source_tweet_id', 'uid',\n",
    "    'description', 'location',\n",
    "    'protected', 'geo_enabled', 'verified',\n",
    "    'followers_count', 'friends_count', 'listed_count', 'favorites_count', 'statuses_count',\n",
    "    'cascade_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../exported/user_profile_train.csv\", header = True, index = False, quoting = csv.QUOTE_NONNUMERIC, quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open(\"../exported/user_profile_test.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        if line[:5] != '\"poli':\n",
    "            print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_df = pd.read_csv(\"../exported/train_ids.csv\", header = 0)\n",
    "cv_id_df = pd.read_csv(\"../exported/cv_ids.csv\", header = 0)\n",
    "test_id_df = pd.read_csv(\"../exported/test_ids.csv\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "historical_texts = []\n",
    "avg_rt_counts = []\n",
    "avg_favorite_counts = []\n",
    "sensitive_ratios = []\n",
    "\n",
    "for idx, row in train_id_df.iterrows():\n",
    "    \n",
    "    recent_posts = []\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "    \n",
    "    with open(f\"{NEWS_PATH}/{row['news_id']}/tweets/{row['source_tweet_id']}.json\", \"r\") as f:\n",
    "        t = json.loads(f.read())\n",
    "        tid = t['id']\n",
    "        u = t['user']\n",
    "        \n",
    "        with open(f\"{DATASET_BASE}/user_timeline_tweets/{u['id']}.json\", \"r\") as ff:\n",
    "            try:\n",
    "                posts = json.loads(ff.read())\n",
    "            except:\n",
    "                historical_texts.append(\"\")\n",
    "                avg_rt_counts.append(-1)\n",
    "                avg_favorite_counts.append(-1)\n",
    "                sensitive_ratios.append(-1)\n",
    "                continue\n",
    "            \n",
    "            total_n = len(posts)\n",
    "            target_at = -1\n",
    "            \n",
    "            for i, p in enumerate(posts):\n",
    "                if tid == p['id']:\n",
    "                    target_at = i\n",
    "                    break\n",
    "            \n",
    "            if target_at != -1:\n",
    "                if (total_n - 1 - target_at) >= K:\n",
    "                    recent_posts = posts[target_at+1:target_at+1+K]\n",
    "            elif total_n >= K:\n",
    "                recent_posts = posts[total_n-K:]\n",
    "            else:\n",
    "                recent_posts = posts\n",
    "\n",
    "            if len(recent_posts):\n",
    "                concat_text = \"\"\n",
    "                rt_counts = []\n",
    "                favorite_counts = []\n",
    "                sensitive_count = 0\n",
    "                for p in recent_posts:\n",
    "                    concat_text += p['text']\n",
    "                    rt_counts.append(p['retweet_count'])\n",
    "                    favorite_counts.append(p['favorite_count'])\n",
    "                    if \"possibly_sensitive\" in p and p['possibly_sensitive']:\n",
    "                        sensitive_count += 1\n",
    "\n",
    "                historical_texts.append(concat_text)\n",
    "                avg_rt_counts.append(statistics.mean(rt_counts))\n",
    "                avg_favorite_counts.append(statistics.mean(favorite_counts))\n",
    "                sensitive_ratios.append(sensitive_count / len(recent_posts))\n",
    "            else:\n",
    "                historical_texts.append(\"\")\n",
    "                avg_rt_counts.append(-1)\n",
    "                avg_favorite_counts.append(-1)\n",
    "                sensitive_ratios.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, t in enumerate(historical_texts):\n",
    "    historical_texts[idx] = t.replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_df['text'] = historical_texts\n",
    "train_id_df['avg_rt_count'] = avg_rt_counts\n",
    "train_id_df['avg_favorite_count'] = avg_favorite_counts\n",
    "train_id_df['sensitive_ratio'] = sensitive_ratios\n",
    "\n",
    "train_df = train_id_df[['news_id', 'source_tweet_id',\n",
    "                      'text', 'avg_rt_count', 'avg_favorite_count', 'sensitive_ratio',\n",
    "                      'cascade_size']]\n",
    "\n",
    "train_df.to_csv(\"../exported/user_timeline_train.csv\", header = True, index = False, quoting = csv.QUOTE_NONNUMERIC, quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open(\"../exported/user_timeline_test.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        if line[:5] != '\"poli':\n",
    "            print(idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
